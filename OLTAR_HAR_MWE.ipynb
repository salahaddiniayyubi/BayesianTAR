{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(rf\"C:\\Users\\samir\\Desktop\\TAR-DT\\a_close_final\\data\\data_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['timestamp', 'rv_d', 'rv_w', 'rv_m', 'returns', 'volume']\n",
    "data.set_index(\"timestamp\", inplace=True)\n",
    "data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data[['rv_d', 'rv_w', 'rv_m']].to_numpy()\n",
    "rv_d = data_num[:,0]\n",
    "rv_w = data_num[:, 1]\n",
    "rv_m = data_num[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def evaluate_performance_har(rv_d, rv_m, rv_w, x):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a binary classifier using HAR model in each regime.\n",
    "\n",
    "    Args:\n",
    "        rv_d (np.ndarray): Daily realized volatility (target and predictor).\n",
    "        rv_m (np.ndarray): Monthly realized volatility.\n",
    "        rv_w (np.ndarray): Weekly realized volatility.\n",
    "        x (np.ndarray): Binary classifier predictions (regime indicators).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (ssr, phi)\n",
    "            ssr (float): Sum of squared residuals from the HAR model.\n",
    "            phi (np.ndarray): Estimated coefficients for both regimes.\n",
    "    \"\"\"\n",
    "    T = len(rv_d)\n",
    "    \n",
    "    # Create design matrix Y for HAR model\n",
    "    Y = np.column_stack((np.ones(T-1), rv_d[:-1], rv_w[:-1], rv_m[:-1]))\n",
    "    \n",
    "    # Target variable\n",
    "    y = rv_d[1:]\n",
    "    \n",
    "    # Estimate coefficients for two regimes (k=0 and k=1)\n",
    "    phi = np.zeros((2, 4))\n",
    "    for k in range(2):\n",
    "        mask = (x == k)\n",
    "        Y_k = Y[mask]\n",
    "        y_k = y[mask]\n",
    "        if len(y_k) > 0:\n",
    "            phi[k] = np.linalg.lstsq(Y_k, y_k)[0]\n",
    "    \n",
    "    # Calculate and return sum of squared residuals\n",
    "    residuals = y - np.sum(Y * phi[x], axis=1)\n",
    "    ssr = np.sum(residuals**2)\n",
    "    \n",
    "    return ssr, phi\n",
    "\n",
    "# # Create a single realization of binary regime array x (with values 0 or 1)\n",
    "# x = np.random.choice([0, 1], size=len(rv_d)-1)\n",
    "# print(x)\n",
    "# # Test the evaluate_performance_har function\n",
    "# ssr, coefficients = evaluate_performance_har(rv_d,rv_w, rv_m, x)\n",
    "\n",
    "# print(f\"Sum of Squared Residuals: {ssr}\")\n",
    "# print(f\"Coefficients:\\nRegime 0: {coefficients[0]}\\nRegime 1: {coefficients[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "def estimate_hidden_states(rv_d, rv_m, rv_w, q, N, rho, omega, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Estimate hidden states using a genetic algorithm approach.\n",
    "\n",
    "    Args:\n",
    "        rv_d (np.ndarray): Daily realized volatility.\n",
    "        rv_m (np.ndarray): Monthly realized volatility.\n",
    "        rv_w (np.ndarray): Weekly realized volatility.\n",
    "        q (int): Lag order for the regression.\n",
    "        N (int): Number of samples to generate in each iteration.\n",
    "        rho (float): Quantile threshold for selecting elite samples.\n",
    "        omega (float): Mixing factor for updating probabilities.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (estimated_states, probabilities, best_performance)\n",
    "            estimated_states (np.ndarray): Estimated hidden binary states (0 or 1).\n",
    "            probabilities (np.ndarray): Final probabilities for each state.\n",
    "            best_performance (tuple): Best performance metrics from evaluate_performance_har.\n",
    "    \"\"\"\n",
    "    T = len(rv_d) - q\n",
    "    p = np.full(T, 0.5)  # Initialize probabilities\n",
    "    gammas = [0] * 10\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # Generate samples and evaluate performance\n",
    "        X = bernoulli.rvs(p, size=(N, T))\n",
    "        S = np.array([evaluate_performance_har(rv_d, rv_m, rv_w, x)[0] for x in X])\n",
    "        \n",
    "        # Select elite samples\n",
    "        gamma = np.percentile(S, rho * 100)\n",
    "        gammas.append(gamma)\n",
    "        elite_samples = X[S <= gamma]\n",
    "        \n",
    "        # Update probabilities\n",
    "        p_new = np.mean(elite_samples, axis=0)\n",
    "        p = omega * p_new + (1 - omega) * p\n",
    "        \n",
    "        # Check for convergence\n",
    "        if (np.all(np.abs(p - 0.5) > 0.5 - tol)) or (gammas[-1] == gammas[-2] == gammas[-3] == gammas[-4]):\n",
    "            best_x = X[S == min(S)][0]\n",
    "            estimated_states = np.append([np.nan]*q, (p > 0.5).astype(int))\n",
    "            probabilities = np.append([np.nan]*q, p)\n",
    "            best_performance = evaluate_performance_har(rv_d, rv_m, rv_w, best_x)\n",
    "            return estimated_states, probabilities, best_performance\n",
    "    \n",
    "    # If max_iter is reached without convergence\n",
    "    best_x = X[S == min(S)][0]\n",
    "    estimated_states = np.append([np.nan]*q, (p > 0.5).astype(int))\n",
    "    probabilities = np.append([np.nan]*q, p)\n",
    "    best_performance = evaluate_performance_har(rv_d, rv_m, rv_w, np.append([0]*q, best_x))\n",
    "    return estimated_states, probabilities, best_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informative_states(prob, pi):\n",
    "    \"\"\"\n",
    "    Identify informative states based on probability values and a retention criterion.\n",
    "\n",
    "    This function selects a subset of states considered informative based on their\n",
    "    probability values and a specified retention ratio. It uses entropy as a measure\n",
    "    of informativeness and returns binary indicators for the selected states.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    prob : np.ndarray\n",
    "        An array of probabilities for different states.\n",
    "    pi : float\n",
    "        The retention ratio, determining the proportion of states to keep.\n",
    "    y : np.ndarray\n",
    "        A target array used for determining the number of retained states.\n",
    "    q : integer\n",
    "        The order of the autoregressive process. Due to the loss of the first q observations in AR(q) we have to add them to index.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of binary values indicating informative states, where 1 indicates\n",
    "        the state is considered informative (probability > 0.5) and 0 otherwise.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function uses entropy (prob * (1 - prob)) as a measure of informativeness.\n",
    "    - Lower entropy values are considered more informative.\n",
    "    - The number of retained states is calculated as ceil(pi * len(y)).\n",
    "    \"\"\"\n",
    "    # Calculate the number of states to retain\n",
    "    retained = np.ceil(pi * len(prob)).astype(int)\n",
    "\n",
    "    # Calculate the entropy of each state\n",
    "    entropy = prob * (1 - prob)\n",
    "\n",
    "    # Get the indices of the retained states (lowest entropy)\n",
    "    indices = np.argpartition(entropy, retained)[:retained]\n",
    "\n",
    "    # Sort the indices to maintain original order\n",
    "    sorted_indices = np.sort(indices)\n",
    "\n",
    "    # Return binary indicators for the retained states align it with index of the original series y:\n",
    "    # you have 2 minus q when you want to slice state_est\n",
    "    return np.sort(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Stage - CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming rv_d, rv_m, and rv_w are your input arrays\n",
    "q = 1  # Example lag order\n",
    "N = 60000  # Example number of samples\n",
    "rho = 0.001  # Example quantile threshold\n",
    "omega = 0.7  # Example mixing factor\n",
    "\n",
    "# Use the first 200 observations to get the state estimates for the digression model:\n",
    "estimated_states, probabilities, best_performance = estimate_hidden_states(rv_d, rv_m, rv_w, q, N, rho, omega)\n",
    "\n",
    "# print(\"Estimated States:\", estimated_states)\n",
    "# print(\"Probabilities:\", probabilities)\n",
    "# print(\"Best Performance - SSR:\", best_performance[0])\n",
    "# print(\"Best Performance - Coefficients:\", best_performance[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 200\n",
    "N_valid = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   7,   8,   9,  10,  11,  14,  15,  16,  17,\n",
       "        18,  20,  21,  22,  23,  24,  25,  28,  30,  31,  32,  34,  36,\n",
       "        37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,\n",
       "        50,  51,  52,  53,  54,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "        64,  65,  66,  67,  68,  69,  70,  71,  72,  74,  77,  78,  79,\n",
       "        80,  81,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
       "        94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 105, 106, 107,\n",
       "       108, 109, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
       "       124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138,\n",
       "       140, 141, 142, 143, 144, 147, 148, 149, 150, 151, 153, 155, 157,\n",
       "       158, 159, 161, 164, 165, 166, 167, 168, 169, 172, 173, 174, 175,\n",
       "       176, 178, 179, 180, 181, 184, 185, 186, 187, 191, 192, 193, 194,\n",
       "       196, 197, 198, 199], dtype=int64)"
      ]
     },
     "execution_count": 980,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train = informative_states(probabilities[:N_train], 0.8)\n",
    "indices_valid = informative_states(probabilities[N_train:N_valid], 0.8)+N_train\n",
    "indices_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 1., 0.]))"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retained_states_train = estimated_states[indices_train]\n",
    "retained_states_valid = estimated_states[indices_valid]\n",
    "retained_states_train, retained_states_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run OLS To Get Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "har_features = pd.DataFrame()\n",
    "har_features[\"rv_d\"] = rv_d[:N_train]\n",
    "har_features[\"rv_w\"] = rv_w[:N_train]\n",
    "har_features[\"rv_m\"] = rv_m[:N_train]\n",
    "har_features = har_features.shift(1)\n",
    "\n",
    "# Assuming rv_d is your daily realized volatility series\n",
    "rv_df = pd.Series(rv_d[:N_train])  # Convert to pandas Series if it's not already\n",
    "\n",
    "# Create dummy variables for states\n",
    "# Assuming 'x' is your binary state indicator (0 or 1)\n",
    "dummyv = pd.get_dummies(retained_states_train, prefix='State')\n",
    "\n",
    "# Combine HAR features and dummy variables\n",
    "X = pd.concat([har_features, dummyv], axis=1)\n",
    "X[\"rv_df\"] = rv_df\n",
    "# Prepare the dependent variable\n",
    "for har_col in har_features.columns:\n",
    "    for state_col in dummyv.columns:\n",
    "        X[f'{har_col}_{state_col}'] = X[har_col] * X[state_col]\n",
    "\n",
    "# Add a constant term\n",
    "X = sm.add_constant(X).dropna()\n",
    "y = X[\"rv_df\"]\n",
    "X.drop(columns=[\"rv_df\", \"rv_d\", \"rv_m\", \"rv_w\", \"const\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  rv_df   R-squared:                       0.408\n",
      "Model:                            OLS   Adj. R-squared:                  0.380\n",
      "Method:                 Least Squares   F-statistic:                     14.84\n",
      "Date:                Sun, 06 Oct 2024   Prob (F-statistic):           1.20e-14\n",
      "Time:                        04:52:59   Log-Likelihood:                -463.29\n",
      "No. Observations:                 159   AIC:                             942.6\n",
      "Df Residuals:                     151   BIC:                             967.1\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "State_0.0          0.0488      2.745      0.018      0.986      -5.375       5.473\n",
      "State_1.0          3.1267      1.287      2.430      0.016       0.584       5.669\n",
      "rv_d_State_0.0    -0.1125      0.217     -0.518      0.605      -0.542       0.317\n",
      "rv_d_State_1.0     0.5906      0.086      6.845      0.000       0.420       0.761\n",
      "rv_w_State_0.0     0.9310      0.411      2.264      0.025       0.119       1.743\n",
      "rv_w_State_1.0     0.1485      0.137      1.081      0.281      -0.123       0.420\n",
      "rv_m_State_0.0     0.0928      0.447      0.208      0.836      -0.790       0.976\n",
      "rv_m_State_1.0    -0.2156      0.206     -1.047      0.297      -0.622       0.191\n",
      "==============================================================================\n",
      "Omnibus:                       89.497   Durbin-Watson:                   1.889\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              479.073\n",
      "Skew:                           2.064   Prob(JB):                    9.35e-105\n",
      "Kurtosis:                      10.434   Cond. No.                         85.8\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split and Classification Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create features from the time series y\n",
    "def create_features(y):\n",
    "    name = y.name\n",
    "    features = pd.DataFrame({\n",
    "        rf'{name}_3Yt-3 - 2Y^2_t-2': 3*y.shift(3) - 2*y.shift(2)**2,\n",
    "        rf'{name}_Yt-1': y.shift(1),\n",
    "        rf'{name}_Yt-2': y.shift(2),\n",
    "        rf'{name}_Yt-3': y.shift(3),\n",
    "        rf'{name}_Yt-4': y.shift(4),\n",
    "        rf'{name}_Y^3_t-1 - 13Yt-2 + 14': y.shift(1)**3 - 13*y.shift(2) + 14,\n",
    "        rf'{name}_Y^2_t-2 - 11Yt-3 + 77': y.shift(2)**2 - 11*y.shift(3) + 77,\n",
    "        rf'{name}_Y^3_t-3 - 17Yt-4 + 66': y.shift(3)**3 - 17*y.shift(4) + 66,\n",
    "        rf'{name}_5U[1,0]': np.random.uniform(1, 0, len(y)) * 5,\n",
    "        rf'{name}_2N(0,1) + 3U[0,1]': 2*np.random.normal(0, 1, len(y)) + 3*np.random.uniform(0, 1, len(y)),\n",
    "        rf'{name}_Y^4_t-1 + 2Yt-1 + 8N(0,1)': y.shift(1)**4 + 2*y.shift(1) + 8*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^4_t-3 + 3Yt-3 + 4N(0,1)': y.shift(3)**4 + 3*y.shift(3) + 4*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^3_t-1 + 4Yt-1 + 3N(0,1)': y.shift(1)**3 + 4*y.shift(1) + 3*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^2_t-2 + 5Yt-3 + 6N(0,1)': y.shift(2)**2 + 5*y.shift(3) + 6*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^2_t-3 + 6Yt-3 + 7N(0,1)': y.shift(3)**2 + 6*y.shift(3) + 7*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^2_t-2 + 7Yt-2 + 4N(0,1)': y.shift(2)**2 + 7*y.shift(2) + 4*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^3_t-3 + 8Yt-3 + 8N(0,1)': y.shift(3)**3 + 8*y.shift(3) + 8*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^3_t-3 + 9Yt-3 + 3N(0,1)': y.shift(3)**3 + 9*y.shift(3) + 3*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^3_t-3 + 10Yt-3 + 2N(0,1)': y.shift(3)**3 + 10*y.shift(3) + 2*np.random.normal(0, 1, len(y)),\n",
    "        rf'{name}_Y^2_t-2 + 11Yt-2 + 4N(0,1)': y.shift(2)**2 + 11*y.shift(2) + 4*np.random.normal(0, 1, len(y))\n",
    "    })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_new = data[\"rv_d\"]\n",
    "dat = data.shift(1)\n",
    "dat[\"rv_flag\"] = rv_new\n",
    "dat[\"states\"] = estimated_states\n",
    "# datt = pd.concat([create_features(dat.rv_d), create_features(dat.rv_m), create_features(dat.rv_w), create_features(dat.volume), create_features(dat.returns)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rv_d</th>\n",
       "      <th>rv_w</th>\n",
       "      <th>rv_m</th>\n",
       "      <th>returns</th>\n",
       "      <th>volume</th>\n",
       "      <th>rv_flag</th>\n",
       "      <th>states</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.618131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-01</th>\n",
       "      <td>3.618131</td>\n",
       "      <td>5.078403</td>\n",
       "      <td>6.092091</td>\n",
       "      <td>1.301310</td>\n",
       "      <td>264649.34909</td>\n",
       "      <td>5.385635</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-02</th>\n",
       "      <td>5.385635</td>\n",
       "      <td>4.353663</td>\n",
       "      <td>6.241618</td>\n",
       "      <td>2.593225</td>\n",
       "      <td>310790.42271</td>\n",
       "      <td>7.882679</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03</th>\n",
       "      <td>7.882679</td>\n",
       "      <td>4.930218</td>\n",
       "      <td>6.481974</td>\n",
       "      <td>-1.032249</td>\n",
       "      <td>364177.20751</td>\n",
       "      <td>3.719638</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-04</th>\n",
       "      <td>3.719638</td>\n",
       "      <td>4.649991</td>\n",
       "      <td>6.562119</td>\n",
       "      <td>-0.243133</td>\n",
       "      <td>332571.02904</td>\n",
       "      <td>1.251661</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-29</th>\n",
       "      <td>1.080738</td>\n",
       "      <td>3.603085</td>\n",
       "      <td>5.568050</td>\n",
       "      <td>0.133786</td>\n",
       "      <td>9127.23316</td>\n",
       "      <td>0.990325</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30</th>\n",
       "      <td>0.990325</td>\n",
       "      <td>3.356390</td>\n",
       "      <td>5.387618</td>\n",
       "      <td>-0.389457</td>\n",
       "      <td>8337.74111</td>\n",
       "      <td>4.731435</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>4.731435</td>\n",
       "      <td>3.309243</td>\n",
       "      <td>5.513317</td>\n",
       "      <td>-3.528524</td>\n",
       "      <td>30011.08752</td>\n",
       "      <td>7.738511</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-02</th>\n",
       "      <td>7.738511</td>\n",
       "      <td>3.765209</td>\n",
       "      <td>5.572952</td>\n",
       "      <td>-4.063624</td>\n",
       "      <td>43671.48108</td>\n",
       "      <td>10.352085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-03</th>\n",
       "      <td>10.352085</td>\n",
       "      <td>4.719850</td>\n",
       "      <td>5.747511</td>\n",
       "      <td>-0.257709</td>\n",
       "      <td>31534.70118</td>\n",
       "      <td>6.320025</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 rv_d      rv_w      rv_m   returns        volume    rv_flag  \\\n",
       "timestamp                                                                      \n",
       "2023-01-31        NaN       NaN       NaN       NaN           NaN   3.618131   \n",
       "2023-02-01   3.618131  5.078403  6.092091  1.301310  264649.34909   5.385635   \n",
       "2023-02-02   5.385635  4.353663  6.241618  2.593225  310790.42271   7.882679   \n",
       "2023-02-03   7.882679  4.930218  6.481974 -1.032249  364177.20751   3.719638   \n",
       "2023-02-04   3.719638  4.649991  6.562119 -0.243133  332571.02904   1.251661   \n",
       "...               ...       ...       ...       ...           ...        ...   \n",
       "2024-09-29   1.080738  3.603085  5.568050  0.133786    9127.23316   0.990325   \n",
       "2024-09-30   0.990325  3.356390  5.387618 -0.389457    8337.74111   4.731435   \n",
       "2024-10-01   4.731435  3.309243  5.513317 -3.528524   30011.08752   7.738511   \n",
       "2024-10-02   7.738511  3.765209  5.572952 -4.063624   43671.48108  10.352085   \n",
       "2024-10-03  10.352085  4.719850  5.747511 -0.257709   31534.70118   6.320025   \n",
       "\n",
       "            states  \n",
       "timestamp           \n",
       "2023-01-31     NaN  \n",
       "2023-02-01     1.0  \n",
       "2023-02-02     1.0  \n",
       "2023-02-03     1.0  \n",
       "2023-02-04     1.0  \n",
       "...            ...  \n",
       "2024-09-29     0.0  \n",
       "2024-09-30     1.0  \n",
       "2024-10-01     1.0  \n",
       "2024-10-02     1.0  \n",
       "2024-10-03     1.0  \n",
       "\n",
       "[612 rows x 7 columns]"
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dat = pd.concat([dat, datt], axis = 1).dropna()\n",
    "# dat = data.dropna()\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_retained = dat.iloc[indices_train, ~dat.columns.isin([\"states\", \"rv_flag\"])]\n",
    "y_retained = dat.iloc[indices_train, dat.columns == \"states\"]\n",
    "X_valid    = dat.iloc[N_train:N_valid, ~dat.columns.isin([\"states\", \"rv_flag\"])]\n",
    "y_valid    = dat.iloc[N_train:N_valid, dat.columns==\"states\"]   \n",
    "X_test     = dat.iloc[N_valid:, ~dat.columns.isin([\"states\", \"rv_flag\"])]\n",
    "y_test     = dat.iloc[N_valid:, dat.columns==\"states\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process for Search Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>states</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-08-19</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-20</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-21</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-22</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-04</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            states\n",
       "timestamp         \n",
       "2023-08-19     1.0\n",
       "2023-08-20     0.0\n",
       "2023-08-21     0.0\n",
       "2023-08-22     0.0\n",
       "2023-08-23     0.0\n",
       "...            ...\n",
       "2024-03-01     1.0\n",
       "2024-03-02     1.0\n",
       "2024-03-03     0.0\n",
       "2024-03-04     1.0\n",
       "2024-03-05     0.0\n",
       "\n",
       "[160 rows x 1 columns]"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.iloc[indices_valid-N_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py:1588\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py:3903\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3896\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3898\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3903\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py:3887\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3887\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3889\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3892\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3893\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\managers.py:966\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_indices:\n\u001b[1;32m--> 966\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    968\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexers\\utils.py:287\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 287\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [999]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clfs[best_alpha_idx]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fit the optimal tree\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m optimal_tree \u001b[38;5;241m=\u001b[39m \u001b[43mfit_optimal_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_retained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_retained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices_valid\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mN_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices_valid\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mN_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 3: Forecast the test set\u001b[39;00m\n\u001b[0;32m     31\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m optimal_tree\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Input \u001b[1;32mIn [999]\u001b[0m, in \u001b[0;36mfit_optimal_tree\u001b[1;34m(X_retained, y_retained, X_valid, y_valid)\u001b[0m\n\u001b[0;32m     13\u001b[0m     clfs\u001b[38;5;241m.\u001b[39mappend(clf)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Validate and choose the best alpha\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m acc_scores \u001b[38;5;241m=\u001b[39m [accuracy_score(y_valid\u001b[38;5;241m.\u001b[39miloc[indices_valid\u001b[38;5;241m-\u001b[39mN_train], clf\u001b[38;5;241m.\u001b[39mpredict(X_valid\u001b[38;5;241m.\u001b[39miloc[indices_valid\u001b[38;5;241m-\u001b[39mN_train])) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m clfs]\n\u001b[0;32m     17\u001b[0m best_alpha_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(acc_scores)\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "Input \u001b[1;32mIn [999]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m     clfs\u001b[38;5;241m.\u001b[39mappend(clf)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Validate and choose the best alpha\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m acc_scores \u001b[38;5;241m=\u001b[39m [accuracy_score(\u001b[43my_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices_valid\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mN_train\u001b[49m\u001b[43m]\u001b[49m, clf\u001b[38;5;241m.\u001b[39mpredict(X_valid\u001b[38;5;241m.\u001b[39miloc[indices_valid\u001b[38;5;241m-\u001b[39mN_train])) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m clfs]\n\u001b[0;32m     17\u001b[0m best_alpha_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(acc_scores)\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py:1074\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1071\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1073\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py:1617\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1621\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexing.py:1591\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Step 2: Fit decision tree using cost-complexity pruning and time series validation\n",
    "def fit_optimal_tree(X_retained, y_retained, X_valid, y_valid):\n",
    "    # Fit the initial tree\n",
    "    clf = DecisionTreeClassifier(random_state=42, criterion=\"entropy\", class_weight={0:1, 1:2})\n",
    "    path = clf.cost_complexity_pruning_path(X_retained, y_retained)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "    \n",
    "    # Train trees with different alphas\n",
    "    clfs = []\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha, criterion=\"entropy\")\n",
    "        clf.fit(X_retained, y_retained)\n",
    "        clfs.append(clf)\n",
    "    \n",
    "    # Validate and choose the best alpha\n",
    "    acc_scores = [accuracy_score(y_valid.iloc[indices_valid-N_train], clf.predict(X_valid.iloc[indices_valid-N_train])) for clf in clfs]\n",
    "    best_alpha_idx = np.argmax(acc_scores)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plot_tree(clfs[best_alpha_idx], \n",
    "          feature_names=X_retained.columns,  \n",
    "          class_names=[\"0\", \"1\"],\n",
    "          filled=True, \n",
    "          rounded=True)\n",
    "    plt.show()\n",
    "    return clfs[best_alpha_idx]\n",
    "\n",
    "# Fit the optimal tree\n",
    "optimal_tree = fit_optimal_tree(X_retained, y_retained, X_valid.iloc[indices_valid-N_train], y_valid.iloc[indices_valid-N_train])\n",
    "\n",
    "# Step 3: Forecast the test set\n",
    "y_pred_test = optimal_tree.predict(X_test)\n",
    "\n",
    "# Print feature importances\n",
    "importances = pd.Series(optimal_tree.feature_importances_, index=X_valid.columns).sort_values(ascending=False)\n",
    "print(\"Feature Importances:\")\n",
    "print(importances)\n",
    "\n",
    "# Print test set accuracy (assuming you have true labels for the test set)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Set Accuracy: {test_accuracy}\")\n",
    "\n",
    "# If you don't have true labels for the test set, you can just print the predictions\n",
    "print(\"Test Set Predictions:\")\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
